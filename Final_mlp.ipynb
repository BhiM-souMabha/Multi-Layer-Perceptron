{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a15bf3-713e-453d-b4ea-e9bf4350bfce",
   "metadata": {},
   "source": [
    "# Multi Layered Perceptron\n",
    "\n",
    "**Soumabha Bhim**  \n",
    "June 22, 2024\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Write your abstract here.\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The multilayer perceptron is a feedforward network...\n",
    "\n",
    "---\n",
    "\n",
    "## Images\n",
    "\n",
    "![Image A Caption](example-image-a){ width=45% }\\ \n",
    "![Image B Caption](example-image-b){ width=45% }\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "Describe your methodology here.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Summarize your findings and conclusions here.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "List your references here.\n",
    "\n",
    "---\n",
    "\n",
    "**A project submitted in partial fulfillment of the requirements for the degree of XYZ at ABC University.**\n",
    "\n",
    "**Supervisor:** Dr. John Doe\n",
    "\n",
    "**Department of Computer Science**  \n",
    "**ABC University**  \n",
    "**City, Country**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cda10f21-ea15-4670-acd6-fa621e8406a2",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron\n",
    "\n",
    "The multilayer perceptron is a feedforward network (meaning that the data flows from the input layer to the output layer) defined by the presence of one or more hidden layers as well as an interconnection of all the neurons of one layer to the next.\n",
    "\n",
    "![Example of a MLP having two hidden layers](im.png)\n",
    "Figure 1: Example of a MLP having two hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed6ebc-1488-4b04-b4fa-41fb95962a91",
   "metadata": {},
   "source": [
    "## Input layer\n",
    "The input layer of an MLP receives input data, which could be features extracted from the input samples in a dataset. Each neuron in the input layer represents one feature.\n",
    "Neurons in the input layer do not perform any computations; they simply pass the input values to the neurons in the first hidden layer.\n",
    "\n",
    "## Hidden layers\n",
    "The hidden layers of an MLP consist of interconnected neurons that perform computations on the input data.\n",
    "Each neuron in a hidden layer receives input from all neurons in the previous layer. The inputs are multiplied by corresponding weights. The weights determine how much influence the input from one neuron has on the output of another.\n",
    "In addition to weights, each neuron in the hidden layer has an associated bias. The bias provides an additional input to the neuron, allowing it to adjust its output threshold. Like weights, biases are learned during training.\n",
    "For each neuron in a hidden layer or the output layer, the weighted sum of its inputs is computed. This involves multiplying each input by its corresponding weight, summing up these products, and adding the bias.\n",
    "\n",
    "## Output layer\n",
    "The output layer of an MLP produces the final predictions or outputs of the network. The number of neurons in the output layer depends on the task being performed (e.g., binary classification, multi-class classification, regression).\n",
    "Each neuron in the output layer receives input from the neurons in the last hidden layer and applies an activation function. This activation function is usually different from those used in the hidden layers and produces the final output value or prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804ff2a-4214-4099-bc4b-3971024cd575",
   "metadata": {},
   "source": [
    "![Composition](im2.png)\n",
    "Figure 2: Composition of Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f8cd29-7243-47aa-b4d8-836acc4dba4e",
   "metadata": {},
   "source": [
    "##  How does Multilayer Perceptron work? \n",
    "\n",
    "We can summarize the operation of the perceptron as follows it:\n",
    "\n",
    "  - **Step 1**: Initialize the weights and bias with small-randomized values;\n",
    "  - **Step 2**: Propagate all values in the input layer until output layer(Forward Propagation)\n",
    "  - **Step 3**: Update weight and bias in the inner layers(Backpropagation)\n",
    "  - **Step 4**: Do it until that the stop criterion is satisfied !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036e7dd-3cc6-428f-adcf-b503a4ad71c9",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "The idea is that a unit gets \"activated\" in more or less the same manner that a neuron gets activated when a sufficiently strong input is received. The selection of a sigmoid is arbitrary. Many different non-linear functions could be selected at this stage in the network, like a Tanh or a ReLU. Unfortunately, there is no principled way to chose activation functions for hidden layers. It is mostly a matter of trial and error.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1192/1*4ZEDRpFuCIpUjNgjDdT2Lg.png\">\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "Backpropagation is the learning mechanism that allows the Multilayer Perceptron to iteratively adjust the weights in the network, with the goal of minimizing the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aba95f-edc6-4f93-8e81-304cd5250f09",
   "metadata": {},
   "source": [
    "# An Analysis about Iris Dataset\n",
    "\n",
    "In the first place, Let's define some libraries to help us in the manipulation the data set, such as `numpy`, `pandas`.Here we are implementing a Multilayer Perceptron with Keras framework.\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3a7d67-1f43-412f-b098-cb03d55718af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "Iris_data = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f729bd3-32c9-4f06-959d-add87e917852",
   "metadata": {},
   "source": [
    "### Dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4ea791-38ac-4b07-b60a-2377f122ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "|details-start|\n",
      "**References**\n",
      "|details-split|\n",
      "\n",
      "- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "  Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "  Structure and Classification Rule for Recognition in Partially Exposed\n",
      "  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "  on Information Theory, May 1972, 431-433.\n",
      "- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "  conceptual clustering system finds 3 classes in the data.\n",
      "- Many, many more ...\n",
      "\n",
      "|details-end|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Iris_data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef524e-ffd6-411e-8125-a29659736da9",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29876235-a903-4782-8b51-18641a830312",
   "metadata": {},
   "source": [
    "Since neural networks aren’t designed with the use of human-friendly class labels in mind, we need to encode the species into numeric values.We use one-hot encoding in which the output is transformed from a single value into one feature (column) for each possible output class where the appropriate class has a value of 1 (hot) and all others are 0. This way, we can train the model to classify observations without any implied relationships between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac756cf-41a2-469c-8944-824f22a19286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset\n",
    "df = pd.read_csv(\"Iris.csv\")\n",
    "# One-hot encoding\n",
    "y = pd.get_dummies(df[\"Species\"])\n",
    "# drop the species and id column\n",
    "x = df.drop([\"Id\",\"Species\"],axis=1)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821a11e-7e37-4af6-80b0-ba1fa4ee9a74",
   "metadata": {},
   "source": [
    "### The network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be38e608-802f-44b8-a796-0a9847711f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# creating our model --> MULTILAYER PERCEPTRON\n",
    "model = Sequential()\n",
    "model.add(Dense(10,activation=\"sigmoid\"))  # Hidden layer\n",
    "model.add(Dense(3,activation=\"softmax\")) # Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9703606-c7f6-440f-a495-3421db3e3abe",
   "metadata": {},
   "source": [
    " Here, our model consists of a sequence of two Dense layers, which are densely connected (also called fully connected) neural layers. The second (and last) layer is a 3-way softmax classification layer, which means it will return an array of 3 probability scores\n",
    " (summing to 1). Each score will be the probability that the current inputs\n",
    " belongs to one of our 3 species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87610d-def3-4b5d-a762-cf0cc677a96c",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc70b03-8be6-45e8-ba3c-a502028b7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])\n",
    "model.fit(x_train,y_train,epochs= 25 , batch_size= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470caf75-1177-479f-9309-128b1c1a4496",
   "metadata": {},
   "source": [
    "Two quantities are displayed during training: the loss of the model over the training data, and the accuracy of the model over the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d412fbc-37f6-4e48-b345-66e574c22d6c",
   "metadata": {},
   "source": [
    "### Evaluate the model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06a0e923-11fa-4716-ac48-23d330e4fe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8738 - loss: 0.6636  \n",
      "Loss: 0.6543\n",
      "Accuracy: 0.8889\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "score = model.evaluate(x_test, y_test)\n",
    "loss, accuracy = score[0], score[1]\n",
    "\n",
    "print(f\"Loss: {loss:0.4f}\")\n",
    "print(f\"Accuracy: {accuracy:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f761ca-3e23-4d04-ac90-57952fd20000",
   "metadata": {},
   "source": [
    " The test-set accuracy turns out to be 95.56%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603cea8c-02c9-42ce-8955-dc1b4655f05d",
   "metadata": {},
   "source": [
    "### Using the model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a8c89d-8f9d-41d6-b2e2-466f949fd831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6890196 , 0.1815414 , 0.12943901]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = np.array([[5,3.3,1.4,0.2]])\n",
    "i = model.predict(predict)\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea7839-e615-456b-ae17-6a00b9397873",
   "metadata": {},
   "source": [
    "This output indicates the probabilities assigned by the model to each of the three classes. Specifically:\n",
    "\n",
    "*   **Setosa -** 92.25%\n",
    "*   **Versicolor -** 7.16%\n",
    "*   **Virginica -** 0.59%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
